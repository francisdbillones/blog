<!doctype html><meta charset=utf-8><meta name=author content="Francis Billones"><meta name=description content="Neural networks are closer to matrices than actual brains."><meta charset=utf-8><meta name=viewport content="width=device-width"><meta property="og:image" content="https://www.blog.francisdb.net/assets/logo.svg"><meta property="og:description" content="Neural networks are closer to matrices than actual brains."><meta property="og:title" content="Neural nets are non-linear linear transformations"><meta property="og:url" content="https://blog.francisdb.net/articles/Neural nets are non-linear linear transformations.md"><meta property="og:type" content="website"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@francishubertdb"><meta name=twitter:creator content="@francishubertdb"><meta name=twitter:title content="Neural nets are non-linear linear transformations"><meta name=twitter:description content="Neural networks are closer to matrices than actual brains."><meta name=twitter:image content="https://www.blog.francisdb.net/assets/logo.svg"><title>Neural nets are non-linear linear transformations</title><link rel=icon href=https://blog.francisdb.net/assets/logo.svg type=image/svg+xml><style>html{margin:0;padding:0}body{color:#ccc;font-weight:400;font-family:sans-serif;font-size:1.1em;line-height:1.6;padding:38px 16px 0;margin:0 auto;max-width:800px;background-attachment:fixed;background-image:radial-gradient(#333 10%,transparent 10%),radial-gradient(#333 10%,transparent 10%);background-color:#000;background-position:0 0,50px 50px;background-size:50px 50px}footer{font-size:80%;color:#aaa;text-align:center;padding:.7em;margin:10px auto;background-color:#000;width:fit-content}header#main-header{border-radius:1em;display:flex;justify-content:space-between}header#main-header,main{padding:1.9em 2em 1.6em;margin-bottom:2em;border-radius:2em;background-color:#1b1b1b}.logo:hover g{fill:#fc3f4e}nav a{color:#fff;margin-left:2em}nav a.active{color:#8da6ff}nav a.active:hover,h1 a:hover{color:#333;text-shadow:0 0 .5em #0002}a{color:#8da6ff;text-decoration:none;outline:0}a:hover{color:rgba(255,255,255,.507);text-shadow:0 0 .5em rgba(255,255,255,.288);text-decoration:none}p{padding:0;margin:.5em 0 1.5em;word-wrap:break-word}h1{font-size:3em;letter-spacing:-.08em;line-height:1.05;font-weight:700;padding:0;margin:32px 0;color:#fff}h1 a{text-decoration:none;color:#fff}h2{margin:2em 0 1em;padding:0;font-size:150%;font-weight:700;letter-spacing:-.05em}h3{margin:2em 0 1em;padding:0;font-size:100%;font-weight:700;letter-spacing:-.05em}h4{font-size:80%;margin:0}blockquote{position:relative;margin:3em 0 1.5em 2em;line-height:1.6em;font-style:italic;font-weight:400;text-indent:24px}blockquote:before{display:block;content:"\201C";font-size:72px;position:absolute;left:-40px;top:0;color:#ccc}img{border:0}.content img{width:100%}.content video{width:100%}ul{list-style-type:square}li{margin-top:.4em;font-weight:400}header#content-header{margin:1em 0 2em;font-size:90%;opacity:.7}div.archive{font-size:90%;clear:both;text-align:center}ul.archive{color:#eee;list-style:none;margin:0;padding:0}span.archiveDate,span.archiveCount{font-size:90%;color:#999;width:8em;display:inline-block}ol{padding:0 0 0 1em}code{font-family:Consolas,Monaco,DejaVu Sans Mono,Bitstream Vera Sans Mono,monospace;background-color:rgba(255,255,255,.7);border-radius:.5em;font-size:80%;padding:.4em;color:#000}pre{font-family:Consolas,Monaco,DejaVu Sans Mono,Bitstream Vera Sans Mono,monospace;font-size:90%;padding:6px 12px;overflow:auto;background-color:rgba(255,255,255,.5);border-radius:.5em;color:#222}pre code{padding:0;background-color:initial}code span.comment{color:#666}code span.string{color:#6a0}code span.regexp{color:#719}code span.punct{color:#17b}code span.keyword{color:#f0b}code span.number{color:#d83}code span.var{color:#209}code span.def{color:#923}@media only screen and (max-width:576px){body{font-size:15px}h1{font-size:2em}}code{white-space:pre-wrap}span.smallcaps{font-variant:small-caps}span.underline{text-decoration:underline}div.column{display:inline-block;vertical-align:top;width:50%}div.hanging-indent{margin-left:1.5em;text-indent:-1.5em}ul.task-list{list-style:none}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><header id=main-header><a href=https://francisdb.net class=logo><img height=30px width=30px src=https://blog.francisdb.net/assets/logo.svg alt="A logo combining the letters 'F' and 'H'" type=image/svg+xml><nav class=menu><a href=https://francisdb.net/>Home</a><a href=https://blog.francisdb.net/ class=active>Blog</a></nav></header><main><header id=content-header><div><strong>Francis Billones, <a href=http://twitter.com/francishubertdb>@francishubertdb</a></strong></div><div>— Saturday, May 14, 2022</div></header><article><h1 id=neural-nets-are-non-linear-linear-transformations>Neural nets are non-linear linear transformations</h1><h2 id=traditional-teaching>Traditional teaching</h2><p><strong><em>Skip to the next section if you’re unfamiliar with the underlying neural network math</em></strong><p>The neural network is a fundamental idea that powers many AI systems we have today. It has been scaled up to tackle hard problems such as natural language understanding, object recognition, driving, and many other abilities thought to be exclusive only to humans.<p>(<em>The idea that intelligence is exclusive to a carbon-based neural network such as the brain is absurd. Any sort of computation that the brain performs to transform electrical signals into coherent thought can and will be done by silicon. The problem is figuring out the computations in the first place.</em>)<p>Tangent aside, the following is a multi-layer feedforward neural network as it is commonly visualized, and how they’re commonly explained: <img src="https://blog.francisdb.net/assets/images/nn.png"><br> A single “neuron” in this network has a “weight” and a “bias”. The neuron takes in a real-valued number as input, and transforms it according to the following equation: <span class="math inline">\(n(x) = n_{w} \cdot x + n_{b}\)</span> (<em>see how it’s a linear equation? This is important for later)</em>. It then feeds the result to the next layer.<p>Thinking about neural networks this way is messy. Instead, we take a different route. Instead, let’s think about the input as being a vector, and we pass this vector input to each layer as a whole, instead of thinking about passing separate numbers to each neuron.<p>Then, if you work out the math, you’ll realize that the linear transformations being performed by each neuron, if you consider them as a whole, is analogous to a matrix vector product. In fact, it is <em>exactly</em> a matrix vector product - we aggregate the weight vectors in each neuron to one large weight matrix, and each bias in each neuron to one large bias vector. Then, the computation being done by a single layer can be represented elegantly with this one-liner: <span class="math inline">\(l(x) = l_W \cdot x + l_b\)</span> - where <span class="math inline">\(l_W\)</span> is the weight matrix, and <span class="math inline">\(l_b\)</span> is the bias vector.<p>Then, neural networks can be simply thought of as a series of linear transformations to data!<h2 id=why-linear-transformations-are-useful>Why linear transformations are useful</h2><p>A linear transformation is a function that takes in a coordinate, and maps it to a different coordinate in Euclidean space. However, the transformation is only classified as linear if grid lines remain <strong>parallel</strong> and <strong>evenly spaced</strong>.<p>Matrices are linear transformations - performing a matrix vector product with a 2x2 matrix on a 2-vector results in another 2-vector. 2-vectors are just vectors with 2 dimensions; therefore, you can extend the idea of a coordinate to a 2-vector.<p>We can combine linear transformations to perform prediction on data.<p>For example, take the following scatter plot: <img src="https://blog.francisdb.net/assets/images/plot1.png""><br><p>If we wanted to come up with our own series of linear transformations to categorise this data, we might come up with the following: <ol><li><strong>Rotation</strong> Imagine the linear boundary between both groups. We rotate the data such that that linear boundary is now horizontal: <img src="https://blog.francisdb.net/assets/images/plot2.png"><br></li><li><strong>Translation</strong> Now, with the rotation being done, our next transformation is not actually a linear transformation, but rather a translation along the y-axis such that both groups are separated by the y-axis, through the use of a bias vector instead of a weight matrix: <img src="https://blog.francisdb.net/assets/images/plot3.png"></li><li> “<strong>Squeeze</strong>” Now, we use a singular matrix to transform the data from 2D Euclidean space to 1D Euclidean space, preserving the y-coordinate: <img src="https://blog.francisdb.net/assets/images/plot4.png"><br></br> Now, our hypothetical neural network’s outputs can be easily interpretable - anything less than 0 is part of the “red” class, and anything above 0 is part of the “blue” class.<p>These three linear transformations don’t need to be in separate layers, either - they can be easily combined to one layer, by multiplying them all: <span class="math inline">\(L = l_3 \cdot l_2 \cdot l_1\)</span>.<h2 id=but-how-about-more-complex-data></li></ol><h2>But how about more complex data?</h2><p>Not all data is linearly separable by default. For example, observe this scatter plot: [[spiral.png]] There is no obvious linear pattern we can observe here. Therefore, we have to <strong>make</strong> our linear transformations <strong>non-linear</strong>, and we do that by interspersing them with nonlinear functions: <span class="math display">\[l(x) = \sigma(l_W \cdot x + l_b)\]</span> where <span class="math inline">\(\sigma\)</span> is any arbitrary nonlinear function.<p>With this added functionality, here’s a visualization of how a nonlinear function can <strong>transform</strong> nonlinear data into linear data:<p><img src="https://blog.francisdb.net/assets/images/spiral.gif"><p>Linearly separable data is much easier to deal with!<h3 id=conclusion>Conclusion</h3><p>In this article, we showed that neural networks (at least, the feed-forward neural network) are not what they are commonly compared to - biological neural networks. Instead, they are a series of linear transformations in the form of matrix vector products, interspersed with nonlinearities.<p>This fundamental idea of composing “non-linear linear transformations” has led to many breakthroughs in the field of AI; namely, <a href=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf>convolutional neural networks</a> in the field of computer vision, <a href=https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>transformer networks</a> in the field of natural language understanding, and <a href=http://www.bioinf.jku.at/publications/older/2604.pdf>LSTMs</a> in the field of general time-series forecasting. With parameter counts reaching up to the hundreds of <strong>billions</strong>, it is not hard to see that this basic idea of stacking transformations can tackle much higher-dimensional data such as images and natural language.</article></main><footer>© 2022 Francis Billones, loaded <strong>11.03KB</strong></footer>